<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Nathan Cornille</title>
    <link>https://nathancornille.github.io/posts/</link>
    <description>Recent content in Posts on Nathan Cornille</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 17 Mar 2022 00:00:00 +0100</lastBuildDate>
    
	    <atom:link href="https://nathancornille.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Taking automatic deconfounding for out-of-distribution performance under the loop</title>
      <link>https://nathancornille.github.io/posts/p1/</link>
      <pubDate>Thu, 17 Mar 2022 00:00:00 +0100</pubDate>
      
      <guid>https://nathancornille.github.io/posts/p1/</guid>
      <description>&lt;p&gt;This blog post will give a less technical explanation of the main ideas from my (first!) paper &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/frai.2022.736791/full&#34;&gt;Critical Analysis of Deconfounded Pretraining to Improve Visio-Linguistic Models&lt;/a&gt;.
Let&amp;rsquo;s dive right into it.&lt;/p&gt;
&lt;h3 id=&#34;the-goal-out-of-distribution-performance&#34;&gt;The goal: out-of-distribution performance&lt;/h3&gt;
&lt;p&gt;Machine learning has improved a lot in its ability to learn an accurate predictive model of various kinds of data.
Specifically for data that is visual (in the form of pixels) or linguistic (in the form of natural language) great strides have been made.
A big challenge that remains however, is to predict well when the distribution of data on which we train differs from the one on which we want to make predictions: this is called out-of-distribution (or OOD) prediction.
One proposed way of addressing this challenge, is by using &lt;strong&gt;causal&lt;/strong&gt; models.&lt;/p&gt;
&lt;h3 id=&#34;causality-and-ood&#34;&gt;Causality and OOD&lt;/h3&gt;
&lt;p&gt;You can find a more detailed explanation as well as an example of the link between causal models and OOD prediction in &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/frai.2022.736791/full#h4&#34;&gt;section 3 of the paper&lt;/a&gt;.
In a nutshell though, in a causal model you chop up (or &amp;lsquo;factorize&amp;rsquo;) the relations in your data into pieces that follow lines of the underlying causality.
For example, when modelling the presence of rain and umbrellas in the street, you&amp;rsquo;ll model 1) the probability of rain (P&lt;sub&gt;R&lt;/sub&gt;), 2) the probability of umbrellas &lt;em&gt;given&lt;/em&gt; rain (P&lt;sub&gt;U|R&lt;/sub&gt;).
The alternative would be to model 1) the probability of umbrellas (P&lt;sub&gt;U&lt;/sub&gt;) 2) the probability of rain given umbrellas (P&lt;sub&gt;R|U&lt;/sub&gt;).
The link with OOD prediction then comes from the Sparse Mechanism Shift (SMS) assumption proposed by Bernard Sch√∂lkopf and others: this assumption states that differences in distribution typically correspond to the change in only a few causal mechanisms.
For example, when we change our distribution by looking at data from a more rainy country, if you factorized the relations causally, only P&lt;sub&gt;R&lt;/sub&gt; will change.
If you factorized them the other way however, both factors P&lt;sub&gt;U&lt;/sub&gt; and P&lt;sub&gt;R|U&lt;/sub&gt; will change.&lt;/p&gt;
&lt;h3 id=&#34;deconfounding&#34;&gt;Deconfounding&lt;/h3&gt;
&lt;p&gt;Now, several papers have tried to use this reasoning to create models that are better at OOD prediction.
A particular class of papers that I take under the loop here, are papers that try to automatically do deconfounding in order to learn more causal models.
Deconfounding is a technique to adjust for the effect of a common cause when investigating the relation between two variables.
For example, the prevalence of storks, and the prevalence of new babies is correlated in European countries.
This can mean three things: storks cause babies (by carrying them in), babies cause storks (maybe they attract them in some way), &lt;strong&gt;or&lt;/strong&gt; a common cause has an influence on both (say, the socio-economic status of a country, which has an influence both on how much nature there is, as on the birthrates).
If we look at the relation between babies and storks &lt;em&gt;for countries with the same socio-economic status)&lt;/em&gt;, we might well find the relation disappears.&lt;/p&gt;
&lt;h3 id=&#34;issues-with-automatic-deconfounding&#34;&gt;Issues with automatic deconfounding&lt;/h3&gt;
&lt;p&gt;The tricky thing about doing deconfounding, is that you need to know what is a correct confounder (in our example: socio-economic status) in the first place.&lt;/p&gt;
&lt;p&gt;Researchers that have made use of the technique that I take under the loop (which I call &lt;em&gt;AutoDeconfounding&lt;/em&gt; or AD), assume it can automatically discover correct confounders just from looking at data.
However, this hasn&amp;rsquo;t been tested explicitly, so we create a dataset to test whether true confounders are actually found.&lt;/p&gt;
&lt;p&gt;Moreover, the link between AD and theoretically correct deconfounding hadn&amp;rsquo;t been made explicit yet, so we formalize the assumptions that need to hold for the implementation of AD to match correct deconfounding.&lt;/p&gt;
&lt;p&gt;Finally, what &lt;em&gt;has&lt;/em&gt; been shown, is that AD improves OOD prediction performance. However, we provide evidence that the improved performance observed in previous experiments isn&amp;rsquo;t actually due to AD.&lt;/p&gt;
&lt;h3 id=&#34;our-results&#34;&gt;Our results&lt;/h3&gt;
&lt;h4 id=&#34;are-confounders-found&#34;&gt;Are confounders found?&lt;/h4&gt;
&lt;p&gt;To answer this question, we need ground-truth data of what variables are confounders.
As a confounder is by definition a cause of two other variables, we need to know which variables cause which.
In our case, the variables aren&amp;rsquo;t storks and babies, but the presence of objects in a scene.
The ideal way to test whether a causal relation, is by doing an experiment (e.g., a randomized controlled trial): put an object in a scene (say a raincloud), and observe whether some other object (an umbrella) subsequently appears or not.
As we deemed it too hard to do this kind of live experimentation (it is tricky to drag rainclouds in front of a camera), we went for an approximation instead: human judgement.
We asked crowdworkers to judge the relation between the presence of objects, and took that as our ground truth.&lt;/p&gt;
&lt;p&gt;We found that the confounder-finding mechanism &lt;em&gt;did&lt;/em&gt; in fact outperform a random baseline in correctly distinguishing causes from non-causal correlates.
However, we found that doing better at confounder-finding did not correlate with a better OOD performance.&lt;/p&gt;
&lt;h4 id=&#34;is-ad-equivalent-with-deconfounding&#34;&gt;Is AD equivalent with deconfounding?&lt;/h4&gt;
&lt;p&gt;The actual implementation of AD is somewhat far removed from the theoretical principle.
We made the link more clear by making the assumptions explicit under which the implementation matches the theory.
The assumptions come down to a need for the encoding of the confounder variables to be powerful enough to represent their full state, and the need for the confounders to be mutually independent.&lt;/p&gt;
&lt;h4 id=&#34;does-ad-improve-ood-performance&#34;&gt;Does AD improve OOD performance?&lt;/h4&gt;
&lt;p&gt;For our final result, we redo the original experiments that investigate the effect of AD.
We compare a model using AD with a baseline model that doesn&amp;rsquo;t use it. In contrast to previous work, we don&amp;rsquo;t just copy the results from the baseline models as they were originally reported, but reproduce both the AD model as the baseline model ourselves, to get a like-for-like comparison.
In this like-for-like comparison, &lt;em&gt;we no longer observe the improvement in performance&lt;/em&gt;.
Moreover, we create a few ablated models, which change AD such as to be less related to theoretical deconfounding, and find that these too don&amp;rsquo;t perform worse than the original AD model.&lt;/p&gt;
&lt;h3 id=&#34;so-what&#34;&gt;So what?&lt;/h3&gt;
&lt;p&gt;Our investigation has mainly been a critical story: what things are &lt;em&gt;not&lt;/em&gt; yet as they should be.
I think this is how science progresses however: sometimes you advance the field by proposing something new, sometimes you do it by correcting an existing theory.
Hence, I hope this will enable other researchers who are interested in leveraging causality for OOD performance to know which techniques to use, and which techniques still have their limitations.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
