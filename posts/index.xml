<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Nathan Cornille</title>
    <link>https://nathancornille.github.io/posts/</link>
    <description>Recent content in Posts on Nathan Cornille</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 02 Aug 2024 00:00:00 +0200</lastBuildDate>
    
	    <atom:link href="https://nathancornille.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Learning to Plan for Language Modeling from Unlabeled Data</title>
      <link>https://nathancornille.github.io/posts/colm_2024/</link>
      <pubDate>Fri, 02 Aug 2024 00:00:00 +0200</pubDate>
      
      <guid>https://nathancornille.github.io/posts/colm_2024/</guid>
      <description>&lt;!---
GOAL
 - ? Make website which currently only has blog post for first paper up-to-date?
    - but that would imply a blog post for second paper also
 - Have a more accessible presentation of the paper ideas that will engage people seeing my twitter/linkedin post about the paper more
STRUCTURE
 - Problem setting
    - LMs are very useful, and a self-supervised pretraining objective is key, because it is scalable
    - Standard LMs still simply predict the next word left-to-right, which seems counter to how humans would write more complex texts: we think ahead at an abstract level about what we want to say, and then write based on this plan
    - our goal was to train a planner to make abstract future writing plans, with properties:
        - being self-supervised, so it is scalable
        - in latent space rather than text space, as it is richer
- Approach
  - main figure
  - 3 parts
    - generating abstract writing plans in a self-supervised way
        - we generate sentence embeddings, and cluster those to have a finite set of what we will call &#39;writing actions&#39;
    - training a planner to predict these writing actions
        - we train a planner to predict the next writing action given the preceding text
          - we found a planner that first contextualizes within-sentence to produce an embedding per sentence, and then across sentences, to work well
    - training an LM to use predicted writing actions to improve perplexity
        - we try two variants: one trained on oracle actions (where we might expect a train/test mismatch), and one trained on predicted actions (where the actions will be less informative about the future text)
- Results
  - Evaluation metrics
    - our main metric of interest is perplexity: the typical metric for LMs
    - We also evaluate generation: both surface-level metrics (ROUGE-2 measuring n-gram overlap with true text, and MAUVE comparing the true and predict distribution for open-ended text generation), and abstract-level metrics: converting true and generate text into a sequence of their corresponding high-level writing actions, and comparing these sequences (with Edit distance and Latent Perplexity). Details about the metrics can be found in the paper.
  - our main goal was to see whether the LM could benefit from the planner predictions
    - because we start with models pre-trained on a different dataset than our fine-tuning data (which is English Wikipedia), we need to control for the effect of adaptation. We do so by comparing to a model that is fine-tuned in a similar way, but while only receiving fixed (and so uninformative) writing actions
    - we find that
      - indeed we can consistently outperform this Fixed/Fixed baseline in both perplexity and generation metrics
      - training the LM on predicted actions benefits perplexity, which makes sense given the lack of train/test mismatch
      - training on oracle actions leads the LM to &#34;follow its own plan&#34; more often. Following your plan means that the cluster of the sentence you generated corresponds to the writing actions used to condition the generation on. This is around 40% for oracle actions compared to 20% for predicted actions.
      - despite having higher perplexity, training the LM on oracle actions is sometimes better for the generation metrics. We hypothesize this is due to its superior plan-following capacity. Finetuning the LM in such a way that it prevents a train/test mismatch _and_ more closely follows the plan is a promising direction for future work.
- What now?
  - While our results are encouraging, our planner might not take the world by storm just yet. For one, it is currently only planning one step ahead. If we can learn to plan multiple steps ahead, we could benefit from search algorithms during generation. For another, the planner-predicted actions might benefit from being tailored to a specific LM. Lots of room for future work!

--&gt;
&lt;p&gt;In this blog post, I&amp;rsquo;ll give a bite-sized summary of our paper &lt;a href=&#34;https://arxiv.org/pdf/2404.00614&#34;&gt;Learning to Plan for Language Modeling from Unlabeled Data&lt;/a&gt;, which was accepted at COLM 2024  ðŸ¥³.&lt;/p&gt;
&lt;h2 id=&#34;problem-setting&#34;&gt;Problem Setting&lt;/h2&gt;
&lt;p&gt;Language Models (LMs) have revolutionized various NLP tasks, largely due to their self-supervised pretraining objectives that scale effectively with data. However, traditional LMs predict the next word in a sequence purely left-to-right, which contrasts with how humans approach writing complex texts. We typically plan ahead, coming up with ideas at an abstract level before putting them into words.&lt;/p&gt;
&lt;p&gt;Our research aimed to bridge this gap by developing a planner that can generate abstract writing plans. These plans have two key features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Scalability through self-supervision&lt;/strong&gt;: Like modern LMs, the planner can learn from large amounts of unlabeled data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Expressiveness by operating in a latent space&lt;/strong&gt;: Instead of planning in text space, which can be restrictive, our planner operates in a more expressive latent space.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;approach&#34;&gt;Approach&lt;/h2&gt;
&lt;p&gt;Our approach is divided into three main components, as illustrated below:&lt;/p&gt;
&lt;img src=&#34;https://nathancornille.github.io/colm_main_fig.png&#34; alt=&#34;Main Figure&#34; width=&#34;50%&#34;&gt;
&lt;p&gt;We start by generating sentence embeddings and then clustering these embeddings to define a set of &amp;lsquo;writing actions&amp;rsquo;. These actions serve as the building blocks for abstract writing plans.&lt;/p&gt;
&lt;p&gt;Next, we train a planner to predict the next writing action based on the preceding text. Our experiments showed that a planner that first processes within-sentence context to create sentence embeddings and then operates across sentences performed the best.&lt;/p&gt;
&lt;p&gt;Finally, we integrate the predicted writing actions into the language model. We explored two training variants:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Oracle actions&lt;/strong&gt;: Using ground-truth actions, knowing this could introduce a train/test mismatch.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Predicted actions&lt;/strong&gt;: Using the plannerâ€™s predicted actions, which are less precise but avoid the mismatch.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;h3 id=&#34;evaluation-metrics&#34;&gt;Evaluation Metrics&lt;/h3&gt;
&lt;p&gt;We assessed our approach using several metrics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Perplexity&lt;/strong&gt;: The standard metric for evaluating LMs, reflecting how well the model predicts the next word.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Surface-level generation metrics&lt;/strong&gt;: ROUGE-2 (measuring n-gram overlap with reference text) and MAUVE (comparing distribution of generated versus reference text).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Abstract-level generation metrics&lt;/strong&gt;: We converted both reference and generated text into sequences of high-level writing actions and compared these using Edit distance and Latent Perplexity.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;key-findings&#34;&gt;Key Findings&lt;/h3&gt;
&lt;p&gt;Our primary goal was to determine whether the language model could benefit from the plannerâ€™s predictions. Hereâ€™s what we discovered:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Improved Performance&lt;/strong&gt;: Our models consistently outperformed the baseline (which used fixed, uninformative actions) in both perplexity and generation metrics.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Perplexity Benefits&lt;/strong&gt;: Training the LM on predicted actions led to better perplexity scores, likely due to the absence of train/test mismatch.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Enhanced Plan-Following&lt;/strong&gt;: Training on oracle actions increased the likelihood that the LM would &amp;ldquo;follow its own plan,&amp;rdquo; with around 40% alignment compared to 20% when using predicted actions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generation Quality&lt;/strong&gt;: Despite worse perplexity, models trained on oracle actions sometimes performed better on generation metrics than model trained on predicted actions, likely due to stronger plan-following. This suggests that fine-tuning LMs to minimize train/test mismatch while maintaining plan fidelity is a promising direction for future research.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-now&#34;&gt;What Now?&lt;/h2&gt;
&lt;p&gt;While our findings are encouraging, our planner might not take the world by storm just yet. For one, it currently only planw one step ahead. Extending this to multi-step planning could open the door to leveraging search algorithms during text generation. Additionally, tailoring planner-predicted actions to specific LMs could further enhance performance. Thereâ€™s plenty of exciting work ahead!&lt;/p&gt;
&lt;p&gt;For a deeper dive, check out our full paper at &lt;a href=&#34;https://arxiv.org/pdf/2404.00614&#34;&gt;https://arxiv.org/pdf/2404.00614&lt;/a&gt;.
hugo&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Taking automatic deconfounding for out-of-distribution performance under the loop</title>
      <link>https://nathancornille.github.io/posts/sceps/</link>
      <pubDate>Thu, 17 Mar 2022 00:00:00 +0100</pubDate>
      
      <guid>https://nathancornille.github.io/posts/sceps/</guid>
      <description>&lt;p&gt;This blog post will give a less technical explanation of the main ideas from my (first!) paper &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/frai.2022.736791/full&#34;&gt;Critical Analysis of Deconfounded Pretraining to Improve Visio-Linguistic Models&lt;/a&gt;.
Let&amp;rsquo;s dive right into it.&lt;/p&gt;
&lt;h3 id=&#34;the-goal-out-of-distribution-performance&#34;&gt;The goal: out-of-distribution performance&lt;/h3&gt;
&lt;p&gt;Machine learning has improved a lot in its ability to learn an accurate predictive model of various kinds of data.
Specifically for data that is visual (in the form of pixels) or linguistic (in the form of natural language) great strides have been made.
A big challenge that remains however, is to predict well when the distribution of data on which we train differs from the one on which we want to make predictions: this is called out-of-distribution (or OOD) prediction.
One proposed way of addressing this challenge, is by using &lt;strong&gt;causal&lt;/strong&gt; models.&lt;/p&gt;
&lt;h3 id=&#34;causality-and-ood&#34;&gt;Causality and OOD&lt;/h3&gt;
&lt;p&gt;You can find a more detailed explanation as well as an example of the link between causal models and OOD prediction in &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/frai.2022.736791/full#h4&#34;&gt;section 3 of the paper&lt;/a&gt;.
In a nutshell though, in a causal model you chop up (or &amp;lsquo;factorize&amp;rsquo;) the relations in your data into pieces that follow lines of the underlying causality.
For example, when modelling the presence of rain and umbrellas in the street, you&amp;rsquo;ll model 1) the probability of rain (P&lt;sub&gt;R&lt;/sub&gt;), 2) the probability of umbrellas &lt;em&gt;given&lt;/em&gt; rain (P&lt;sub&gt;U|R&lt;/sub&gt;).
The alternative would be to model 1) the probability of umbrellas (P&lt;sub&gt;U&lt;/sub&gt;) 2) the probability of rain given umbrellas (P&lt;sub&gt;R|U&lt;/sub&gt;).
The link with OOD prediction then comes from the Sparse Mechanism Shift (SMS) assumption proposed by Bernard SchÃ¶lkopf and others: this assumption states that differences in distribution typically correspond to the change in only a few causal mechanisms.
For example, when we change our distribution by looking at data from a more rainy country, if you factorized the relations causally, only P&lt;sub&gt;R&lt;/sub&gt; will change.
If you factorized them the other way however, both factors P&lt;sub&gt;U&lt;/sub&gt; and P&lt;sub&gt;R|U&lt;/sub&gt; will change.&lt;/p&gt;
&lt;h3 id=&#34;deconfounding&#34;&gt;Deconfounding&lt;/h3&gt;
&lt;p&gt;Now, several papers have tried to use this reasoning to create models that are better at OOD prediction.
A particular class of papers that I take under the loop here, are papers that try to automatically do deconfounding in order to learn more causal models.
Deconfounding is a technique to adjust for the effect of a common cause when investigating the relation between two variables.
For example, the prevalence of storks, and the prevalence of new babies is correlated in European countries.
This can mean three things: storks cause babies (by carrying them in), babies cause storks (maybe they attract them in some way), &lt;strong&gt;or&lt;/strong&gt; a common cause has an influence on both (say, the socio-economic status of a country, which has an influence both on how much nature there is, as on the birthrates).
If we look at the relation between babies and storks &lt;em&gt;for countries with the same socio-economic status)&lt;/em&gt;, we might well find the relation disappears.&lt;/p&gt;
&lt;h3 id=&#34;issues-with-automatic-deconfounding&#34;&gt;Issues with automatic deconfounding&lt;/h3&gt;
&lt;p&gt;The tricky thing about doing deconfounding, is that you need to know what is a correct confounder (in our example: socio-economic status) in the first place.&lt;/p&gt;
&lt;p&gt;Researchers that have made use of the technique that I take under the loop (which I call &lt;em&gt;AutoDeconfounding&lt;/em&gt; or AD), assume it can automatically discover correct confounders just from looking at data.
However, this hasn&amp;rsquo;t been tested explicitly, so we create a dataset to test whether true confounders are actually found.&lt;/p&gt;
&lt;p&gt;Moreover, the link between AD and theoretically correct deconfounding hadn&amp;rsquo;t been made explicit yet, so we formalize the assumptions that need to hold for the implementation of AD to match correct deconfounding.&lt;/p&gt;
&lt;p&gt;Finally, what &lt;em&gt;has&lt;/em&gt; been shown, is that AD improves OOD prediction performance. However, we provide evidence that the improved performance observed in previous experiments isn&amp;rsquo;t actually due to AD.&lt;/p&gt;
&lt;h3 id=&#34;our-results&#34;&gt;Our results&lt;/h3&gt;
&lt;h4 id=&#34;are-confounders-found&#34;&gt;Are confounders found?&lt;/h4&gt;
&lt;p&gt;To answer this question, we need ground-truth data of what variables are confounders.
As a confounder is by definition a cause of two other variables, we need to know which variables cause which.
In our case, the variables aren&amp;rsquo;t storks and babies, but the presence of objects in a scene.
The ideal way to test whether a causal relation, is by doing an experiment (e.g., a randomized controlled trial): put an object in a scene (say a raincloud), and observe whether some other object (an umbrella) subsequently appears or not.
As we deemed it too hard to do this kind of live experimentation (it is tricky to drag rainclouds in front of a camera), we went for an approximation instead: human judgement.
We asked crowdworkers to judge the relation between the presence of objects, and took that as our ground truth.&lt;/p&gt;
&lt;p&gt;We found that the confounder-finding mechanism &lt;em&gt;did&lt;/em&gt; in fact outperform a random baseline in correctly distinguishing causes from non-causal correlates.
However, we found that doing better at confounder-finding did not correlate with a better OOD performance.&lt;/p&gt;
&lt;h4 id=&#34;is-ad-equivalent-with-deconfounding&#34;&gt;Is AD equivalent with deconfounding?&lt;/h4&gt;
&lt;p&gt;The actual implementation of AD is somewhat far removed from the theoretical principle.
We made the link more clear by making the assumptions explicit under which the implementation matches the theory.
The assumptions come down to a need for the encoding of the confounder variables to be powerful enough to represent their full state, and the need for the confounders to be mutually independent.&lt;/p&gt;
&lt;h4 id=&#34;does-ad-improve-ood-performance&#34;&gt;Does AD improve OOD performance?&lt;/h4&gt;
&lt;p&gt;For our final result, we redo the original experiments that investigate the effect of AD.
We compare a model using AD with a baseline model that doesn&amp;rsquo;t use it. In contrast to previous work, we don&amp;rsquo;t just copy the results from the baseline models as they were originally reported, but reproduce both the AD model as the baseline model ourselves, to get a like-for-like comparison.
In this like-for-like comparison, &lt;em&gt;we no longer observe the improvement in performance&lt;/em&gt;.
Moreover, we create a few ablated models, which change AD such as to be less related to theoretical deconfounding, and find that these too don&amp;rsquo;t perform worse than the original AD model.&lt;/p&gt;
&lt;h3 id=&#34;so-what&#34;&gt;So what?&lt;/h3&gt;
&lt;p&gt;Our investigation has mainly been a critical story: what things are &lt;em&gt;not&lt;/em&gt; yet as they should be.
I think this is how science progresses however: sometimes you advance the field by proposing something new, sometimes you do it by correcting an existing theory.
Hence, I hope this will enable other researchers who are interested in leveraging causality for OOD performance to know which techniques to use, and which techniques still have their limitations.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
